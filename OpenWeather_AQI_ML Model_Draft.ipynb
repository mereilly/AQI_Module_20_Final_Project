{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84df85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding library dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from config import password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36993c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "# from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sql alchemy / pandas methods (maybe read_sql)\n",
    "# from OpenWeather_AQI_DB_Final import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6263bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating connection between AWS and PostgreSQL database\n",
    "engine = create_engine(f'postgresql://root:{password}@final-project-db.ch7spkcdsndt.us-east-2.rds.amazonaws.com:5432/FinalProjectDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ddfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_aqi_table = engine.execute(\"\"\"SELECT * FROM public.full_aqi_data_table WHERE 'State'='AK'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af75481",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_aqi_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store CSV into DataFrame\n",
    "# Read in data and display first 5 rows\n",
    "AQI_data = pd.read_csv('Resources/AQI_data.csv')\n",
    "AQI_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AQI_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for each column\n",
    "#All cells are full, verifying there aren't gaps/missing values to clean/remove associated values\n",
    "AQI_copy = AQI_data\n",
    "# Labels are the values we want to predict\n",
    "labels = AQI_data['AQI']\n",
    "# Features are the values we want to evaluate in reference to the output label\n",
    "features = AQI_data.drop(['Date', 'AQI', 'State','Latitude', 'Longitude', 'dt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, random_state=1)\n",
    "# test_size = 0.25 or another test size could be added if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the output of the training split\n",
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5665e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Evaluating the output of the testing split\n",
    "features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c459da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling features so that varying data ranges can be comparable \n",
    "scaler = StandardScaler().fit(features_train)\n",
    "features_train_scaled = scaler.transform(features_train)\n",
    "features_test_scaled = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Instantiate model with 600 decision trees\n",
    "rclf = RandomForestClassifier(n_estimators = 600, random_state = 7)\n",
    "# Train the model on training data\n",
    "rclf.fit(features_train_scaled, labels_train)\n",
    "\n",
    "\n",
    "print(f'Training Score: {rclf.score(features_train_scaled, labels_train)}')\n",
    "print(f'Testing Score: {rclf.score(features_test_scaled, labels_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe99e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rclf.predict(features_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - labels_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1411cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = (errors / labels_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating impact importance of the different features (i.e. pollutants relavance on the AQI)\n",
    "feature_importances = rclf.feature_importances_\n",
    "results = sorted(zip(features.columns, rclf.feature_importances_), key = lambda x: x[1])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is to make a visualization with plotly showing the importance of the pollutants on the AQI score\n",
    "\n",
    "cols = [r[0] for r in results]\n",
    "width = [r[1] for r in results]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(5,5)\n",
    "plt.margins(y=0.001)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ae1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Considering another classifier model\n",
    "\n",
    "# X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X_train_features), y_train_labels, random_state=1)\n",
    "# scaler = StandardScaler().fit(X_selected_train)\n",
    "# X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "# X_selected_test_scaled = scaler.transform(X_selected_test)\n",
    "\n",
    "# rclf = LogisticRegression().fit(X_train_features, y_train)\n",
    "# print(f'Training Score: {rclf.score(X_train_features, y_train_labels)}')\n",
    "# print(f'Testing Score: {rclf.score(X_test_features, y_test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query using different fileters from databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Considering AdaBoost CLassifier\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "#   from sklearn.datasets import make_classification\n",
    "#   X, y = make_classification(n_samples=1000, n_features=4,\n",
    "# ...                            n_informative=2, n_redundant=0,\n",
    "# ...                            random_state=0, shuffle=False)\n",
    "#   clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "#   clf.fit(X, y)\n",
    "# AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "#   clf.predict([[0, 0, 0, 0]])\n",
    "# array([1])\n",
    "#   clf.score(X, y)\n",
    "# 0.983..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7efa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy as np\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "# y = np.array([1, 1, 2, 2])\n",
    "# from sklearn.svm import SVC\n",
    "# clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "# clf.fit(X, y)\n",
    "#Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "#                ('svc', SVC(gamma='auto'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1abfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check xgboost version? Cofused by this error, unable to try associated classifier model - see sample below\n",
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test classification dataset\n",
    "# from sklearn.datasets import make_classification\n",
    "# # define dataset\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# # summarize the dataset\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using xgboost for classification\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# define the model\n",
    "model = XGBClassifier()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\n",
    "row = asarray([row])\n",
    "yhat = model.predict(row)\n",
    "print('Predicted Class: %d' % yhat[0])\n",
    "\n",
    "# make predictions using xgboost for classification\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# define the model\n",
    "model = XGBClassifier()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\n",
    "row = asarray([row])\n",
    "yhat = model.predict(row)\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37974fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
